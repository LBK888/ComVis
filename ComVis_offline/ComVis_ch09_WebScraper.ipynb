{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Python 網路爬蟲教學 - 使用 requests 庫**\n",
        "網路爬蟲(Web Scraper)指的是利用程式到網頁、網路上讀取資料  \n",
        "以爬蟲的角度來看分類，可以將網頁資料分為三類:\n",
        "1. 別人希望你爬的: 提供API介面與說明文件，使用可能會收費，但是取得的資料很好處理\n",
        "2. 不希望你爬的: 可以假裝是瀏覽器來取得資料，但後續要再進行資料清理\n",
        "3. 不希望你爬的，且具有高度防護的: 使用真人認證、帳密登入、行為監測，需要高度技術來突破\n",
        "\n",
        "**注意事項:**\n",
        "1. 爬蟲是一個需要深度了解http與html/javascript原理的程式設計，這邊指提供是粗淺的範例\n",
        "2. 爬蟲取得的資料，可能具有版權或是隱私權爭議，請自行注意\n",
        "3. 由於是透過網址取得資料，因此本範例隨時間久遠之後，會因網站關閉、網址改變等原因不能使用\n"
      ],
      "metadata": {
        "id": "E2wu2FrDfqnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 1. 基礎設置與安装套件 =====\n",
        "import requests\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "from IPython.display import clear_output, display, Image\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 設置中文字體（Colab 環境）\n",
        "!wget -O TaipeiSansTCBeta-Regular.ttf https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_&export=download\n",
        "import matplotlib.font_manager as fm\n",
        "fm.fontManager.addfont('TaipeiSansTCBeta-Regular.ttf')\n",
        "plt.rcParams['font.family'] = 'Taipei Sans TC Beta'\n",
        "\n",
        "print(\"✅ 所有套件安裝完成！\")"
      ],
      "metadata": {
        "id": "M7Vc2_HDfnET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4GXZvksZRG_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ===== 2. 基礎概念：什麼是 HTTP 請求？ =====\n",
        "print(\"\\n=== HTTP 請求基礎概念 ===\")\n",
        "\n",
        "# HTTP 狀態碼說明\n",
        "def explain_status_codes():\n",
        "    \"\"\"解釋常見的 HTTP 狀態碼\"\"\"\n",
        "    status_codes = {\n",
        "        200: \"成功 - 請求已成功處理\",\n",
        "        404: \"找不到 - 請求的資源不存在，主機存在但網址錯誤\",\n",
        "        403: \"禁止存取 - 沒有權限存取該資源，被檔了\",\n",
        "        500: \"伺服器錯誤 - 伺服器內部發生錯誤，可能有真人認證所以被檔了\",\n",
        "        503: \"Service Unavailable，可能主機繁忙，可稍後再試\",\n",
        "        429: \"請求過於頻繁 - 被限制存取頻率\"\n",
        "    }\n",
        "\n",
        "    for code, description in status_codes.items():\n",
        "        print(f\"{code}: {description}\")\n",
        "\n",
        "explain_status_codes()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 3. 基本的網路請求 =====\n",
        "print(\"\\n=== 基本網路請求示範 ===\")\n",
        "\n",
        "def basic_request_example():\n",
        "    \"\"\"基本的 GET 請求示範\"\"\"\n",
        "    url = \"https://httpbin.org/get\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        print(f\"狀態碼: {response.status_code}\")\n",
        "        print(f\"回應類型: {response.headers.get('content-type')}\")\n",
        "        print(f\"回應內容預覽: {response.text[:200]}...\")\n",
        "\n",
        "        # 檢查請求是否成功\n",
        "        if response.status_code == 200:\n",
        "            print(\"✅ 請求成功！\")\n",
        "        else:\n",
        "            print(\"❌ 請求失敗！\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"請求發生錯誤: {e}\")\n",
        "\n",
        "basic_request_example()\n"
      ],
      "metadata": {
        "id": "ZZyytrwfZuW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 4. User-Agent 和請求標頭的重要性 =====\n",
        "print(\"\\n=== User-Agent 和請求標頭 ===\")\n",
        "\n",
        "def demonstrate_headers():\n",
        "    \"\"\"示範使用不同的請求標頭\"\"\"\n",
        "    url = \"https://httpbin.org/user-agent\"\n",
        "\n",
        "    # 不使用 User-Agent\n",
        "    response1 = requests.get(url)\n",
        "    print(\"不使用 User-Agent:\")\n",
        "    print(response1.json())\n",
        "\n",
        "    # 使用自定義 User-Agent\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "    response2 = requests.get(url, headers=headers)\n",
        "    print(\"\\n使用自定義 User-Agent:\")\n",
        "    print(response2.json())\n",
        "\n",
        "demonstrate_headers()\n",
        "\n"
      ],
      "metadata": {
        "id": "0rzYczb-Zr4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 5. 處理 JSON 資料 =====\n",
        "print(\"\\n=== JSON 資料處理 ===\")\n",
        "\n",
        "def json_data_example():\n",
        "    \"\"\"示範如何處理 JSON 資料\"\"\"\n",
        "    url = 'https://jsonplaceholder.typicode.com/posts/1'\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            print(\"JSON 資料結構:\")\n",
        "            print(f\"標題: {data['title']}\")\n",
        "            print(f\"內容: {data['body'][:50]}...\")\n",
        "            print(f\"作者 ID: {data['userId']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"處理 JSON 時發生錯誤: {e}\")\n",
        "\n",
        "json_data_example()\n",
        "\n"
      ],
      "metadata": {
        "id": "Nlv_s72TZrRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 6. 錯誤處理和異常管理 =====\n",
        "print(\"\\n=== 錯誤處理示範 ===\")\n",
        "\n",
        "def error_handling_example():\n",
        "    \"\"\"示範完整的錯誤處理\"\"\"\n",
        "    urls = [\n",
        "        \"https://httpbin.org/status/200\",  # 正常\n",
        "        \"https://httpbin.org/status/404\",  # 找不到\n",
        "        \"https://invalid-url-12345.com\"   # 無效網址\n",
        "    ]\n",
        "\n",
        "    for url in urls:\n",
        "        try:\n",
        "            print(f\"\\n測試網址: {url}\")\n",
        "            response = requests.get(url, timeout=5)  # 設置超時時間\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                print(\"✅ 請求成功\")\n",
        "            else:\n",
        "                print(f\"⚠️ 狀態碼: {response.status_code}\")\n",
        "\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(\"❌ 請求超時\")\n",
        "        except requests.exceptions.ConnectionError:\n",
        "            print(\"❌ 連線錯誤\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"❌ 請求錯誤: {e}\")\n",
        "\n",
        "error_handling_example()\n"
      ],
      "metadata": {
        "id": "SvIFnZdfZpZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 7. 下載和顯示圖片1 - 有沒有headers的差異 =====\n",
        "print(\"\\n=== 圖片下載與顯示 ===\")\n",
        "\n",
        "def download_and_display_image(image_url,headers=None):\n",
        "    \"\"\"下載並顯示圖片\"\"\"\n",
        "\n",
        "    try:\n",
        "        if headers:\n",
        "            print(\"\\n有傳入headers\")\n",
        "            response = requests.get(image_url,headers=headers)\n",
        "        else:\n",
        "            print(\"\\n沒有傳入headers\")\n",
        "            response = requests.get(image_url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            # 將圖片資料轉換為 numpy 陣列\n",
        "            image_array = np.asarray(bytearray(response.content), dtype=np.uint8)\n",
        "            image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)\n",
        "\n",
        "            # 轉換色彩空間 (BGR to RGB)\n",
        "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # 顯示圖片\n",
        "            plt.figure(figsize=(6, 4))\n",
        "            plt.imshow(image_rgb)\n",
        "            plt.axis('off')\n",
        "            plt.title('下載的圖片')\n",
        "            plt.show()\n",
        "\n",
        "            print(\"✅ 圖片下載並顯示成功！\")\n",
        "        else:\n",
        "            print(f\"❌ 請求失敗，狀態碼: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 圖片處理錯誤: {e}\")\n",
        "\n",
        "\n",
        "# 使用一個穩定的圖片網址\n",
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b3/Oreochromis-niloticus-Nairobi.JPG/1920px-Oreochromis-niloticus-Nairobi.JPG\"\n",
        "image_url = \"https://i.imgur.com/djsHhnF.png\"\n",
        "print(\"同一個的圖片url抓取，比較有沒有headers的差異\")\n",
        "\n",
        "# 沒有header\n",
        "download_and_display_image(image_url)\n",
        "\n",
        "# 有header，使用自定義 User-Agent\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "download_and_display_image(image_url,headers=headers)\n",
        "\n"
      ],
      "metadata": {
        "id": "jte7ZpaXZntN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 7. 下載和顯示圖片2 =====\n",
        "\n",
        "\n",
        "headers = {'Upgrade-Insecure-Requests':'1','user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36'}\n",
        "#response = requests.get('http://gis.taiwan.net.tw/XMLReleaseALL_public/activity_C_f.json')\n",
        "\n",
        "# Adding verify=False to bypass SSL verification\n",
        "response = requests.get('https://tour.klcg.gov.tw/data/attractions.json', headers=headers, verify=False)\n",
        "\n",
        "#文字編碼問題，\"\\u57fa\\u9686\\u6d77\\u6d0b\\u5ee3\\u5834\"這樣的格式就是unicode沒有被正確編碼\n",
        "print(response.text)\n",
        "print(response.json())\n",
        "\n",
        "#指定編碼\n",
        "#response.encoding = 'big5'\n",
        "response.encoding = 'utf-8'\n",
        "print(response.json())\n",
        "\n",
        "\n",
        "#如何從json中取值\n",
        "#response.json()回傳dict字典型態\n",
        "pic_url=response.json()['attractions'][0]['cover_image']\n",
        "print(pic_url)\n",
        "\n",
        "# 如何顯示抓到的照片\n",
        "resp = requests.get(pic_url, stream=True, verify=False).raw\n",
        "pic = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "pic = cv2.imdecode(pic, cv2.IMREAD_COLOR)\n",
        "\n",
        "plt.imshow(pic)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# 如何儲存抓到的照片\n",
        "with open('pic.png', 'wb') as f:\n",
        "    f.write(requests.get(pic_url, verify=False).content)\n",
        "    print(\"儲存成功，請從左側files下載\")\n"
      ],
      "metadata": {
        "id": "IBrrAlyw0BV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 8. CSV 資料處理 =====\n",
        "print(\"\\n=== CSV 資料處理 ===\")\n",
        "\n",
        "def process_csv_data():\n",
        "    \"\"\"處理線上 CSV 資料\"\"\"\n",
        "    # 使用一個可靠的 CSV 資料源\n",
        "    csv_url = \"https://raw.githubusercontent.com/datasets/population/master/data/population.csv\"\n",
        "\n",
        "    try:\n",
        "        # 直接用 pandas 讀取線上 CSV (推薦使用pandas處理所有類似excel格式的資料)\n",
        "        df = pd.read_csv(csv_url)\n",
        "        print(\"CSV 資料讀取成功！\")\n",
        "        print(f\"資料形狀: {df.shape}\")\n",
        "        print(\"\\n前 5 筆資料:\")\n",
        "        display(df.head())\n",
        "\n",
        "        #find Country code = JPN\n",
        "        print(\"抓出JPN關鍵字\")\n",
        "        print(df[df.eq(\"JPN\").any(axis=1)])\n",
        "\n",
        "        # 簡單的資料分析\n",
        "        if 'Value' in df.columns:\n",
        "            print(f\"\\n人口數據統計:\")\n",
        "            print(f\"總記錄數: {len(df)}\")\n",
        "            print(f\"最大值: {df['Value'].max():,.0f}\")\n",
        "            print(f\"最小值: {df['Value'].min():,.0f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ CSV 處理錯誤: {e}\")\n",
        "\n",
        "process_csv_data()\n"
      ],
      "metadata": {
        "id": "w8hWcn6yZlYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 9. API 參數傳遞 =====\n",
        "print(\"\\n=== API 參數傳遞 ===\")\n",
        "\n",
        "def api_parameters_example():\n",
        "    \"\"\"示範 API 參數的使用\"\"\"\n",
        "    base_url = \"https://httpbin.org/get\"\n",
        "\n",
        "    # 方法 1: 直接在 URL 中加入參數 (http的GET方法)\n",
        "    url_with_params = f\"{base_url}?name=Python&level=advanced\"\n",
        "    response1 = requests.get(url_with_params)\n",
        "\n",
        "    # 方法 2: 使用 params 參數\n",
        "    params = {\n",
        "        'name': 'Python',\n",
        "        'level': 'advanced',\n",
        "        'language': '中文'\n",
        "    }\n",
        "    response2 = requests.get(base_url, params=params)\n",
        "\n",
        "    print(\"方法 1 - URL 中的參數:\")\n",
        "    print(response1.json()['args'])\n",
        "\n",
        "    print(\"\\n方法 2 - params 參數:\")\n",
        "    print(response2.json()['args'])\n",
        "\n",
        "api_parameters_example()\n",
        "\n"
      ],
      "metadata": {
        "id": "m28hyc86Zj9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 10. 匯率查詢實例 =====\n",
        "print(\"\\n=== 匯率查詢實例 ===\")\n",
        "\n",
        "def currency_exchange_example():\n",
        "    \"\"\"匯率查詢和歷史資料視覺化\"\"\"\n",
        "    try:\n",
        "        # 取得當前匯率\n",
        "        url = \"https://api.exchangerate-api.com/v4/latest/TWD\"\n",
        "        response = requests.get(url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            print(f\"基準貨幣: {data['base']}\")\n",
        "            print(f\"更新日期: {data['date']}\")\n",
        "\n",
        "            # 顯示主要貨幣匯率\n",
        "            major_currencies = ['EUR', 'GBP', 'JPY', 'CNY', 'USD', 'KRW', 'THB']\n",
        "            print(\"\\n主要貨幣匯率 (以台幣為基準):\")\n",
        "            print(\"-\"*20)\n",
        "            for currency in major_currencies:\n",
        "                if currency in data['rates']:\n",
        "                    print(f\"1 TWD = {data['rates'][currency]:.4f} {currency}\")\n",
        "                    print(f\"1 {currency} = {1/data['rates'][currency]:.4f} TWD\")\n",
        "                    print(\"-\"*20)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 匯率查詢錯誤: {e}\")\n",
        "\n",
        "currency_exchange_example()\n"
      ],
      "metadata": {
        "id": "UXGyKHLaZitJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 11. 網站狀態監控 =====\n",
        "print(\"\\n=== 網站狀態監控 ===\")\n",
        "\n",
        "def website_monitoring():\n",
        "    \"\"\"監控多個網站的狀態\"\"\"\n",
        "    websites = [\n",
        "        \"https://www.google.com\",\n",
        "        \"https://www.github.com\",\n",
        "        \"https://www.stackoverflow.com\"\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for site in websites:\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            response = requests.get(site, timeout=10)\n",
        "            response_time = time.time() - start_time\n",
        "\n",
        "            results.append({\n",
        "                'Website': site.replace('https://www.', ''),\n",
        "                'Status': response.status_code,\n",
        "                'Response Time': f\"{response_time:.2f}s\",\n",
        "                'Available': '✅' if response.status_code == 200 else '❌'\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            results.append({\n",
        "                'Website': site.replace('https://www.', ''),\n",
        "                'Status': 'Error',\n",
        "                'Response Time': 'N/A',\n",
        "                'Available': '❌'\n",
        "            })\n",
        "\n",
        "    # 建立結果 DataFrame\n",
        "    df = pd.DataFrame(results)\n",
        "    print(\"網站狀態監控結果:\")\n",
        "    display(df)\n",
        "\n",
        "website_monitoring()\n"
      ],
      "metadata": {
        "id": "ZedoV4sEZfJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 12. 應用: 即時路況影像 - 照片模式 =====\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy.random import randn\n",
        "from time import sleep\n",
        "\n",
        "#高速公路監視攝影機\n",
        "cam_url='https://cctvn.freeway.gov.tw/abs2mjpg/jpg?camera=10000'\n",
        "\n",
        "for i in range(5):\n",
        "  resp = requests.get(cam_url, stream=True).raw\n",
        "  pic = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "  pic = cv2.imdecode(pic, cv2.IMREAD_COLOR)\n",
        "\n",
        "  clear_output()\n",
        "  plt.imshow(pic)\n",
        "  plt.show()\n",
        "  sleep(2)\n",
        "\n"
      ],
      "metadata": {
        "id": "7WmI3bTUzx0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 12. 應用2: 即時路況影像 - 影片模式 =====\n",
        "\n",
        "#基隆路口監視器\n",
        "cam_url = \"https://cctv.klcg.gov.tw/47668ccf\"\n",
        "\n",
        "cap = cv2.VideoCapture(cam_url)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(\"無法打開影片串流\")\n",
        "else:\n",
        "  for i in range(20):\n",
        "    ret, frame = cap.read()\n",
        "    if ret:\n",
        "        # OpenCV 讀取的影像是 BGR 格式，需要轉換為 RGB 才能用 matplotlib 顯示\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        plt.imshow(frame_rgb)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"無法讀取影像幀\")\n",
        "\n",
        "  cap.release()"
      ],
      "metadata": {
        "id": "_doJm4mL6ig7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**=== 爬蟲禮儀和最佳實踐 ===**\n",
        "1. 尊重 robots.txt: 檢查網站的 robots.txt 檔案，遵守爬蟲規則\n",
        "2. 適當的延遲: 在請求之間加入延遲，避免對伺服器造成過大負擔\n",
        "3. 使用適當的 User-Agent: 不要偽裝成其他爬蟲或瀏覽器\n",
        "4. 處理錯誤: 妥善處理網路錯誤和異常狀況\n",
        "5. 遵守法律: 確保爬蟲行為符合法律規範和網站條款\n",
        "6. 快取機制: 避免重複請求相同的資料"
      ],
      "metadata": {
        "id": "U1mweIgyedec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 14. 簡單的延遲和頻率控制 =====\n",
        "def polite_crawler_example():\n",
        "    \"\"\"有禮貌的爬蟲示範\"\"\"\n",
        "    urls = [\n",
        "        \"https://httpbin.org/delay/1\",\n",
        "        \"https://httpbin.org/delay/1\",\n",
        "        \"https://pytorch.org/\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n=== 有禮貌的爬蟲示範 ===\")\n",
        "\n",
        "    for i, url in enumerate(urls, 1):\n",
        "        print(f\"正在請求第 {i} 個網址...\")\n",
        "\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            response = requests.get(url, timeout=10)\n",
        "            end_time = time.time()\n",
        "\n",
        "            print(f\"✅ 完成，耗時: {end_time - start_time:.2f} 秒\")\n",
        "\n",
        "            # 在請求之間加入延遲（好習慣）\n",
        "            if i < len(urls):\n",
        "                print(\"⏳ 等待 2 秒...\")\n",
        "                time.sleep(2)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ 錯誤: {e}\")\n",
        "\n",
        "polite_crawler_example()\n"
      ],
      "metadata": {
        "id": "A1HUVn9tZSvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===== 13. Session 的使用 =====\n",
        "print(\"\\n=== Session 使用示範 ===\")\n",
        "\n",
        "def session_example():\n",
        "    \"\"\"示範 Session 的使用（保持連線和 cookies）\"\"\"\n",
        "    # 建立 session\n",
        "    session = requests.Session()\n",
        "\n",
        "    # 設置 session 的預設標頭\n",
        "    session.headers.update({\n",
        "        'User-Agent': 'Python Learning Bot 1.0'\n",
        "    })\n",
        "\n",
        "    # 使用 session 進行多次請求\n",
        "    urls = [\n",
        "        \"https://httpbin.org/headers\",\n",
        "        \"https://httpbin.org/user-agent\"\n",
        "    ]\n",
        "\n",
        "    for url in urls:\n",
        "        try:\n",
        "            response = session.get(url)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                print(f\"URL: {url}\")\n",
        "                if 'headers' in data:\n",
        "                    print(f\"User-Agent: {data['headers'].get('User-Agent', 'Not found')}\")\n",
        "                elif 'user-agent' in data:\n",
        "                    print(f\"User-Agent: {data['user-agent']}\")\n",
        "                print(\"-\" * 50)\n",
        "        except Exception as e:\n",
        "            print(f\"錯誤: {e}\")\n",
        "\n",
        "    # 關閉 session\n",
        "    session.close()\n",
        "\n",
        "session_example()\n"
      ],
      "metadata": {
        "id": "MSvkWhPJZSnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**=== 學習總結 ===**\n",
        "\n",
        "📚 核心概念:\n",
        "  • HTTP 請求和回應的基本概念  \n",
        "  • 狀態碼的意義和處理方式  \n",
        "  • JSON 資料格式的解析和使用  \n",
        "  • 錯誤處理和異常管理  \n",
        "  \n",
        "📚 實用技巧:  \n",
        "  • 使用適當的 User-Agent 和請求標頭  \n",
        "  • 參數傳遞的不同方法  \n",
        "  • 圖片和檔案的下載處理  \n",
        "  • Session 的使用時機  \n",
        "  \n",
        "📚 最佳實踐:  \n",
        "  • 遵守爬蟲禮儀    \n",
        "  • 實施適當的延遲控制  \n",
        "  • 完善的錯誤處理機制  \n",
        "  • 資料的有效儲存和處理  \n",
        "  \n",
        "🎉 恭喜！您已經完成了 Python 網路爬蟲的基礎學習！  \n",
        "💡 建議下一步學習: BeautifulSoup (HTML 解析) 和 Selenium (動態網頁爬取)  "
      ],
      "metadata": {
        "id": "amiz4KA3fIYT"
      }
    }
  ]
}