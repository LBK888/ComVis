{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eV_g5fNzWSnm"
   },
   "source": [
    "https://www.kaggle.com/datasets/zalando-research/fashionmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1741836437109,
     "user": {
      "displayName": "廖柏凱",
      "userId": "03363123573677576177"
     },
     "user_tz": -480
    },
    "id": "lbGOx0j_O69Q",
    "outputId": "c88017ed-0b89-4208-bd6c-cf1c364a7f9b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "# 加載數據集\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True)\n",
    "images = trainset.data[20:30]  # 取前10張圖像\n",
    "print(len(trainset.data))\n",
    "\n",
    "# 可視化\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(images[i], cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "executionInfo": {
     "elapsed": 341960,
     "status": "ok",
     "timestamp": 1741837107630,
     "user": {
      "displayName": "廖柏凱",
      "userId": "03363123573677576177"
     },
     "user_tz": -480
    },
    "id": "FPopngjVOURH",
    "outputId": "11589f51-4939-42bf-f96d-fded771889b5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 數據加載與預處理\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 定義CNN模型\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        # We will dynamically calculate the correct size in the forward method\n",
    "        self.fc1 = nn.Linear(0, 120)\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2, 2)\n",
    "\n",
    "        # Dynamically calculate the size of the flattened tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # If fc1 is not initialized with the correct input size, initialize it now\n",
    "        if self.fc1.in_features == 0:\n",
    "            self.fc1 = nn.Linear(x.shape[1], 120)\n",
    "\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 訓練模型\n",
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Lists to store metrics for plotting\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "epochs=5\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate and store metrics for this epoch\n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "# Plotting the metrics\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Testing the model\n",
    "testset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # Disable gradient calculation during testing\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "error",
     "timestamp": 1741837487983,
     "user": {
      "displayName": "廖柏凱",
      "userId": "03363123573677576177"
     },
     "user_tz": -480
    },
    "id": "AGhagvQrleaW",
    "outputId": "d3546288-45f5-4a62-82b1-4e8064dedab7"
   },
   "outputs": [],
   "source": [
    "# prompt: 請將前面訓練完成的model儲存下來。並且寫一段程式可以讀取test.jpg來辨識是該圖是屬於哪一種fashion MNIST的分類。要注意的是需要先將test.jpg轉換成與fashion MNIST相同的輸入格式\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'fashion_mnist_model.pth')\n",
    "\n",
    "# Load the saved model\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Before loading the state_dict, perform a dummy forward pass with a sample input\n",
    "# to initialize the fc1 layer with the correct input size\n",
    "with torch.no_grad():\n",
    "    dummy_input = torch.randn(1, 1, 28, 28) # Create a dummy input tensor\n",
    "    model(dummy_input) # Perform a forward pass to initialize fc1\n",
    "\n",
    "model.load_state_dict(torch.load('fashion_mnist_model.pth')) # Now load the state_dict\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Define the transformation for the input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Ensure the image is grayscale\n",
    "    transforms.Resize((28, 28)),  # Resize to 28x28\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load and preprocess the image\n",
    "try:\n",
    "  image = Image.open('test.jpg')\n",
    "  image = transform(image)\n",
    "  image = image.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "  # Make a prediction\n",
    "  with torch.no_grad():\n",
    "    output = model(image)\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "  # Print the predicted class\n",
    "  print(f\"Predicted class: {predicted.item()}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "  print(\"Error: test.jpg not found.\")\n",
    "except Exception as e:\n",
    "  print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "executionInfo": {
     "elapsed": 167,
     "status": "ok",
     "timestamp": 1741838451095,
     "user": {
      "displayName": "廖柏凱",
      "userId": "03363123573677576177"
     },
     "user_tz": -480
    },
    "id": "dR-ushuRm9UV",
    "outputId": "9834d36b-4fdd-4016-a591-21f2a14e560f"
   },
   "outputs": [],
   "source": [
    "# prompt: 請將前面訓練完成的model儲存下來。並且寫一段程式可以讀取test.jpg來辨識是該圖是屬於哪一種fashion MNIST的分類。要注意的是需要先將test.jpg轉換成與fashion MNIST相同的輸入格式\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "# 定義CNN模型\n",
    "class SimpleCNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        # We will dynamically calculate the correct size in the forward method\n",
    "        self.fc1 = nn.Linear(0, 120)  # Initialize with 0 as before\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2, 2)\n",
    "\n",
    "        # Dynamically calculate the size of the flattened tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # If fc1 is not initialized with the correct input size, initialize it now\n",
    "        if self.fc1.in_features == 0:\n",
    "            #self.fc1 = nn.Linear(x.shape[1], 120)\n",
    "            self.fc1 = nn.Linear(1600, 120)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "# After training, save the model with the correct fc1 size\n",
    "torch.save(model.state_dict(), 'fashion_mnist_model.pth')\n",
    "\n",
    "# Load the saved model\n",
    "model = SimpleCNN2()\n",
    "model.load_state_dict(torch.load('fashion_mnist_model.pth')) # Load the state_dict\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Define the transformation for the input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Ensure the image is grayscale\n",
    "    transforms.Resize((28, 28)),  # Resize to 28x28\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "# 定義類別文字\n",
    "class_names = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "# Load and preprocess the image\n",
    "try:\n",
    "  image = Image.open('d.jpg')\n",
    "  image = transform(image)\n",
    "  image = image.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "  # Make a prediction\n",
    "  with torch.no_grad():\n",
    "    output = model(image)\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "  # Print the predicted class\n",
    "  print(f\"Predicted class: {predicted.item()}, {class_names[predicted.item()]}\")\n",
    "\n",
    "  # 可視化\n",
    "  plt.figure(figsize=(5, 5))\n",
    "\n",
    "  # Reverse normalization for display\n",
    "  image = image.squeeze(0)  # Remove batch dimension\n",
    "  image = image / 2 + 0.5  # Unnormalize\n",
    "  image = image.numpy()  # Convert to NumPy array\n",
    "\n",
    "  plt.imshow(image, cmap='gray')\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "  print(\"Error: test.jpg not found.\")\n",
    "except Exception as e:\n",
    "  print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
